{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 194 files belonging to 2 classes.\n",
      "Using 156 files for training.\n",
      "Found 1315 files belonging to 2 classes.\n",
      "Using 263 files for validation.\n"
     ]
    }
   ],
   "source": [
    "image_size = (180, 180)\n",
    "batch_size = 32\n",
    "\n",
    "test_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"Dataset/test\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size)   \n",
    "                                                         \n",
    "train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"Dataset/train\",  \n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size)                                                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_data.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(int(labels[i]))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    # Entry block\n",
    "    x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(x)\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for size in [128, 256, 512, 728]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = make_model(input_shape=image_size + (3,), num_classes=2)\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "9/9 [==============================] - 71s 8s/step - loss: 0.5568 - accuracy: 0.7338 - val_loss: 0.6942 - val_accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "9/9 [==============================] - 61s 7s/step - loss: 0.1796 - accuracy: 0.9392 - val_loss: 0.6943 - val_accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "9/9 [==============================] - 57s 6s/step - loss: 0.1396 - accuracy: 0.9468 - val_loss: 0.6946 - val_accuracy: 0.5000\n",
      "Epoch 4/20\n",
      "9/9 [==============================] - 71s 8s/step - loss: 0.1159 - accuracy: 0.9620 - val_loss: 0.6966 - val_accuracy: 0.5000\n",
      "Epoch 5/20\n",
      "9/9 [==============================] - 61s 7s/step - loss: 0.0500 - accuracy: 0.9810 - val_loss: 0.6985 - val_accuracy: 0.5000\n",
      "Epoch 6/20\n",
      "9/9 [==============================] - 58s 6s/step - loss: 0.0691 - accuracy: 0.9658 - val_loss: 0.7013 - val_accuracy: 0.5000\n",
      "Epoch 7/20\n",
      "9/9 [==============================] - 63s 7s/step - loss: 0.0597 - accuracy: 0.9734 - val_loss: 0.7062 - val_accuracy: 0.5000\n",
      "Epoch 8/20\n",
      "9/9 [==============================] - 55s 6s/step - loss: 0.0410 - accuracy: 0.9962 - val_loss: 0.7107 - val_accuracy: 0.5000\n",
      "Epoch 9/20\n",
      "9/9 [==============================] - 54s 6s/step - loss: 0.0479 - accuracy: 0.9848 - val_loss: 0.7089 - val_accuracy: 0.5000\n",
      "Epoch 10/20\n",
      "9/9 [==============================] - 56s 6s/step - loss: 0.0233 - accuracy: 0.9962 - val_loss: 0.7090 - val_accuracy: 0.5000\n",
      "Epoch 11/20\n",
      "9/9 [==============================] - 58s 6s/step - loss: 0.0404 - accuracy: 0.9848 - val_loss: 0.7141 - val_accuracy: 0.5000\n",
      "Epoch 12/20\n",
      "9/9 [==============================] - 56s 6s/step - loss: 0.0310 - accuracy: 0.9848 - val_loss: 0.7285 - val_accuracy: 0.5000\n",
      "Epoch 13/20\n",
      "9/9 [==============================] - 61s 7s/step - loss: 0.1247 - accuracy: 0.9506 - val_loss: 0.7222 - val_accuracy: 0.5000\n",
      "Epoch 14/20\n",
      "9/9 [==============================] - 62s 7s/step - loss: 0.1033 - accuracy: 0.9734 - val_loss: 0.7249 - val_accuracy: 0.5000\n",
      "Epoch 15/20\n",
      "9/9 [==============================] - 59s 7s/step - loss: 0.0478 - accuracy: 0.9810 - val_loss: 0.7246 - val_accuracy: 0.5000\n",
      "Epoch 16/20\n",
      "9/9 [==============================] - 57s 6s/step - loss: 0.0490 - accuracy: 0.9810 - val_loss: 0.7240 - val_accuracy: 0.5000\n",
      "Epoch 17/20\n",
      "9/9 [==============================] - 57s 6s/step - loss: 0.0258 - accuracy: 0.9924 - val_loss: 0.7446 - val_accuracy: 0.5000\n",
      "Epoch 18/20\n",
      "9/9 [==============================] - 57s 6s/step - loss: 0.0387 - accuracy: 0.9886 - val_loss: 0.7272 - val_accuracy: 0.5000\n",
      "Epoch 19/20\n",
      "9/9 [==============================] - 57s 6s/step - loss: 0.0189 - accuracy: 0.9962 - val_loss: 0.7244 - val_accuracy: 0.5000\n",
      "Epoch 20/20\n",
      "9/9 [==============================] - 57s 6s/step - loss: 0.0522 - accuracy: 0.9772 - val_loss: 0.7197 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1400ac940>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.h5\"),\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(\n",
    "    train_data, epochs=epochs, callbacks=callbacks, validation_data=test_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = keras.preprocessing.image.load_img(\n",
    "    \"face.jpg\", target_size=image_size\n",
    ")\n",
    "img_array = keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = predictions[0]\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "print(\n",
    "    \"This image is %.2f percent with mask and %.2f percent without mask.\"\n",
    "    % (100 * (1 - score), 100 * score)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATUS] please wait! calibrating...\n",
      "[STATUS] calibration successfull...\n",
      "[0.4273213]\n",
      "[0.42630643]\n",
      "[0.427011]\n",
      "[0.42664313]\n",
      "[0.4266646]\n",
      "[0.42700797]\n",
      "[0.4273545]\n",
      "[0.4296791]\n",
      "[0.4255554]\n",
      "[0.42610013]\n",
      "[0.42613426]\n",
      "[0.42701417]\n",
      "[0.42726657]\n",
      "[0.42751047]\n",
      "[0.4275899]\n",
      "[0.4277056]\n",
      "[0.4278589]\n",
      "[0.42773297]\n",
      "[0.42760587]\n",
      "[0.42763925]\n",
      "[0.42770585]\n",
      "[0.4284998]\n",
      "[0.42849484]\n",
      "[0.4277301]\n",
      "[0.42758125]\n",
      "[0.42775825]\n",
      "[0.4278952]\n",
      "[0.4276678]\n",
      "[0.4274299]\n",
      "[0.4282749]\n",
      "[0.42820746]\n",
      "[0.4281003]\n",
      "[0.42786804]\n",
      "[0.4280274]\n",
      "[0.42923468]\n",
      "[0.42792016]\n",
      "[0.4272802]\n",
      "[0.4271763]\n",
      "[0.4274613]\n",
      "[0.42623764]\n",
      "[0.42575872]\n",
      "[0.42568278]\n",
      "[0.42582577]\n",
      "[0.42603305]\n",
      "[0.42594272]\n",
      "[0.42689365]\n",
      "[0.42633426]\n",
      "[0.4258082]\n",
      "[0.42828834]\n",
      "[0.4244888]\n",
      "[0.42423993]\n",
      "[0.42412347]\n",
      "[0.42403167]\n",
      "[0.42410865]\n",
      "[0.42423683]\n",
      "[0.42428428]\n",
      "[0.4243215]\n",
      "[0.4247066]\n",
      "[0.4240115]\n",
      "[0.42508173]\n",
      "[0.42559946]\n",
      "[0.42577648]\n",
      "[0.42584276]\n",
      "[0.42560717]\n",
      "[0.42547306]\n",
      "[0.42575487]\n",
      "[0.4256248]\n",
      "[0.42564082]\n",
      "[0.4255979]\n",
      "[0.42548153]\n",
      "[0.4256841]\n",
      "[0.42553812]\n",
      "[0.42557603]\n",
      "[0.42551717]\n",
      "[0.4253709]\n",
      "[0.42548245]\n",
      "[0.42551503]\n",
      "[0.42533046]\n",
      "[0.4254347]\n",
      "[0.42567638]\n",
      "[0.42550862]\n",
      "[0.42546275]\n",
      "[0.42550632]\n",
      "[0.4255367]\n",
      "[0.4253801]\n",
      "[0.42548135]\n",
      "[0.4255312]\n",
      "[0.42551404]\n",
      "[0.42682427]\n",
      "[0.42550877]\n",
      "[0.4255474]\n",
      "[0.42591086]\n",
      "[0.42586082]\n",
      "[0.4255674]\n",
      "[0.42554048]\n",
      "[0.42557976]\n",
      "[0.42544392]\n",
      "[0.42563003]\n",
      "[0.42567718]\n",
      "[0.42572236]\n",
      "[0.42576048]\n",
      "[0.42581508]\n",
      "[0.42556754]\n",
      "[0.42572883]\n",
      "[0.42580292]\n",
      "[0.42579722]\n",
      "[0.42570406]\n",
      "[0.42568284]\n",
      "[0.42577744]\n",
      "[0.42575723]\n",
      "[0.4261176]\n",
      "[0.425772]\n",
      "[0.42575067]\n",
      "[0.42569184]\n",
      "[0.42561623]\n",
      "[0.42566964]\n",
      "[0.42572471]\n",
      "[0.42572218]\n",
      "[0.4256953]\n",
      "[0.42567694]\n",
      "[0.42572325]\n",
      "[0.42580172]\n",
      "[0.42575806]\n",
      "[0.4256795]\n",
      "[0.42565495]\n",
      "[0.42577305]\n",
      "[0.42559358]\n",
      "[0.42561337]\n",
      "[0.4257734]\n",
      "[0.42554066]\n",
      "[0.42559108]\n",
      "[0.4255975]\n",
      "[0.42577562]\n",
      "[0.42602828]\n",
      "[0.4258853]\n",
      "[0.4260726]\n",
      "[0.42602473]\n",
      "[0.4257054]\n",
      "[0.4256596]\n",
      "[0.4256182]\n",
      "[0.42552632]\n",
      "[0.4257005]\n",
      "[0.42577738]\n",
      "[0.42564958]\n",
      "[0.4256978]\n",
      "[0.42556614]\n",
      "[0.42615467]\n",
      "[0.42644924]\n",
      "[0.42693487]\n",
      "[0.42771348]\n",
      "[0.427224]\n",
      "[0.42712623]\n",
      "[0.4268632]\n",
      "[0.42712754]\n",
      "[0.42696756]\n",
      "[0.4262395]\n",
      "[0.42772794]\n",
      "[0.42608413]\n",
      "[0.4259132]\n",
      "[0.42584896]\n",
      "[0.4260308]\n",
      "[0.42590454]\n",
      "[0.42595595]\n",
      "[0.4259537]\n",
      "[0.4261324]\n",
      "[0.42606854]\n",
      "[0.42608255]\n",
      "[0.4258902]\n",
      "[0.42598712]\n",
      "[0.42601895]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    accumWeight = 0.5\n",
    "    camera = cv2.VideoCapture(0)\n",
    "\n",
    "    top, right, bottom, left = 10, 350, 225, 590\n",
    "    num_frames = 0\n",
    "\n",
    "    calibrated = False\n",
    "\n",
    "\n",
    "    while(True):\n",
    "\n",
    "        (grabbed, frame) = camera.read()\n",
    "\n",
    "        frame = imutils.resize(frame, width=700)\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        clone = frame.copy()\n",
    "\n",
    "        (height, width) = frame.shape[:2]\n",
    "\n",
    "        roi = frame[top:bottom, right:left]\n",
    "        \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "\n",
    "\n",
    "        if num_frames < 30:\n",
    "            if num_frames == 1:\n",
    "                print(\"[STATUS] please wait! calibrating...\")\n",
    "            elif num_frames == 29:\n",
    "                print(\"[STATUS] calibration successfull...\")\n",
    "        else:\n",
    "                \n",
    "            face_data = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "            faces_found = face_data.detectMultiScale(gray, minSize = (30,30))\n",
    "            faces_amount_found = len(faces_found)\n",
    "\n",
    "            if faces_amount_found != 0:\n",
    "                for (x,y,w,h) in faces_found:\n",
    "                        img_array = keras.preprocessing.image.img_to_array(clone)\n",
    "                        img_array = tf.expand_dims(img_array, 0)\n",
    "                        \n",
    "                        predictions= model.predict(img_array)\n",
    "                        score = predictions[0] \n",
    "                        \n",
    "                        print(score)\n",
    "                        if(score > 0.43):\n",
    "                            cv2.rectangle(clone, (x,y), \n",
    "                                        (x +h, y+w), \n",
    "                                        (0,0,255), 5)\n",
    "                            cv2.putText(clone, \"No Mask\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)                      \n",
    "        \n",
    "                        else: \n",
    "                            cv2.rectangle(clone, (x,y), \n",
    "                                        (x +h, y+w), \n",
    "                                        (0, 255,0), 5)\n",
    "                            cv2.putText(clone, \"Mask Detected\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "                \n",
    "            cv2.imshow(\"Video Feed\", clone)\n",
    "\n",
    "        num_frames += 1\n",
    "\n",
    "        keypress = cv2.waitKey(1) & 0xFF\n",
    "        if keypress == ord(\"q\"):\n",
    "            break\n",
    "            \n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
